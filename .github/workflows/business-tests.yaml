---
name: "Business Tests"

on:
  pull_request:
    paths-ignore:
      - docs/**
    branches:
      - develop
      - release/**
      - main
  workflow_dispatch:

jobs:
  business-test:
    runs-on: ubuntu-latest
    steps:
      ##############
      ### Set-Up ###
      ##############
      -
        name: Checkout
        uses: actions/checkout@v3
        with:
          submodules: recursive
      -
        name: Set-Up JDK 11
        uses: actions/setup-java@v3.4.0
        with:
          java-version: '11'
          distribution: 'adopt'
          cache: 'maven'
      -
        name: Cache ContainerD Image Layers
        uses: actions/cache@v3
        with:
          path: /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs
          key: ${{ runner.os }}-io.containerd.snapshotter.v1.overlayfs
      -
        name: Set-Up Kubectl
        uses: azure/setup-kubectl@v2.0
      -
        name: Helm Set-Up
        uses: azure/setup-helm@v3.3
        with:
          version: v3.8.1
      -
        name: Create k8s Kind Cluster configuration (kind.config.yaml)
        run: |-
          export MAVEN_REPOSITORY=$(./mvnw help:evaluate -Dexpression=settings.localRepository -q -DforceStdout)
          cat << EOF > kind.config.yaml
          ---
          kind: Cluster
          apiVersion: kind.x-k8s.io/v1alpha4
          nodes:
          - role: control-plane
            extraMounts:
              - hostPath: ${PWD}
                containerPath: /srv/product-edc
              - hostPath: ${MAVEN_REPOSITORY}
                containerPath: /srv/m2-repository
              - hostPath: /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs
                containerPath: /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs
          EOF
      -
        name: Create k8s Kind Cluster
        uses: helm/kind-action@v1.3.0
        with:
          config: kind.config.yaml

      ##############################################
      ### Build and load recent images into KinD ###
      ##############################################
      -
        name: Build edc with Gradle to get latest snapshots
        run: ./gradlew publishToMavenLocal
        working-directory: edc
      -
        name: Build edc-controlplane-postgresql-hashicorp-vault
        run: |-
          ./mvnw -s settings.xml -B -pl .,edc-controlplane/edc-controlplane-postgresql-hashicorp-vault -am package -Dmaven.test.skip=true -Pwith-docker-image
        env:
          GITHUB_PACKAGE_USERNAME: ${{ github.actor }}
          GITHUB_PACKAGE_PASSWORD: ${{ secrets.GITHUB_TOKEN }}
      -
        name: Build edc-dataplane-hashicorp-vault
        run: |-
          ./mvnw -s settings.xml -B -pl .,edc-dataplane/edc-dataplane-hashicorp-vault -am package -Dmaven.test.skip=true -Pwith-docker-image
        env:
          GITHUB_PACKAGE_USERNAME: ${{ github.actor }}
          GITHUB_PACKAGE_PASSWORD: ${{ secrets.CXNG_GHCR_PAT }}
      -
        name: Load images into KinD
        run: |-
          kind get clusters | xargs -n1 kind load docker-image edc-controlplane-postgresql-hashicorp-vault:latest edc-dataplane-hashicorp-vault:latest --name

      ############################################
      ### Prepare And Install Test Environment ###
      ############################################
      -
        name: Define test environment variables
        run: |-
          # Define endpoints
          echo "SOKRATES_DATA_MANAGEMENT_URL=http://sokrates-edc-controlplane:8181/data" | tee -a ${GITHUB_ENV}
          echo "SOKRATES_IDS_URL=http://sokrates-edc-controlplane:8282/api/v1/ids" | tee -a ${GITHUB_ENV}
          echo "SOKRATES_DATA_PLANE_URL=http://sokrates-edc-dataplane:8185/api/public" | tee -a ${GITHUB_ENV}
          echo "PLATO_DATA_MANAGEMENT_URL=http://plato-edc-controlplane:8181/data" | tee -a ${GITHUB_ENV}
          echo "PLATO_IDS_URL=http://plato-edc-controlplane:8282/api/v1/ids" | tee -a ${GITHUB_ENV}
          echo "PLATO_DATA_PLANE_URL=http://plato-edc-dataplane:8185/api/public" | tee -a ${GITHUB_ENV}
      -
        name: Install test environment via Helm
        run: |-
          # Update helm dependencies
          helm dependency update edc-tests/src/main/resources/deployment/helm/all-in-one

          # Install the all-in-one supporting infrastructure environment (daps, vault, pgsql)
          helm install test-environment edc-tests/src/main/resources/deployment/helm/all-in-one \
            --set platoedccontrolplane.image.tag=latest \
            --set sokratesedccontrolplane.image.tag=latest \
            --set platoedcdataplane.image.tag=latest \
            --set sokratesedcdataplane.image.tag=latest \
            --set idsdaps.enabled=true \
            --set platovault.enabled=true \
            --set platopostgresql.enabled=true \
            --set sokratesvault.enabled=true \
            --set sokratespostgresql.enabled=true \
            --set platoedccontrolplane.enabled=false \
            --set platoedcdataplane.enabled=false \
            --set platobackendapplication.enabled=false \
            --set sokratesedccontrolplane.enabled=false \
            --set sokratesedcdataplane.enabled=false \
            --set sokratesbackendapplication.enabled=false \
            --set sokrates-backend-application.persistence.enabled=false \
            --set plato-backend-application.persistence.enabled=false \
            --wait-for-jobs --timeout=120s

          # GH pipelines constrained by cpu, so give helm some time to register all resources \w k8s
          sleep 5s

          # Wait for supporting infrastructure to become ready (control-/data-plane, backend service)
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=idsdaps --timeout=120s || ( kubectl logs -l app.kubernetes.io/name=idsdaps && exit 1 )
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=sokratesvault --timeout=120s || ( kubectl logs -l app.kubernetes.io/name=sokratesvault && exit 1 )
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=platovault --timeout=120s || ( kubectl logs -l app.kubernetes.io/name=platovault && exit 1 )
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=sokratespostgresql --timeout=120s || ( kubectl logs -l app.kubernetes.io/name=sokratespostgresql && exit 1 )
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=platopostgresql --timeout=120s || ( kubectl logs -l app.kubernetes.io/name=platopostgresql && exit 1 )

          # Install the all-in-one Control-/DataPlanes and backend-services
          helm upgrade --install test-environment edc-tests/src/main/resources/deployment/helm/all-in-one \
            --set platoedccontrolplane.image.tag=latest \
            --set sokratesedccontrolplane.image.tag=latest \
            --set platoedcdataplane.image.tag=latest \
            --set sokratesedcdataplane.image.tag=latest \
            --set idsdaps.enabled=true \
            --set platovault.enabled=true \
            --set platopostgresql.enabled=true \
            --set sokratesvault.enabled=true \
            --set sokratespostgresql.enabled=true \
            --set platoedccontrolplane.enabled=true \
            --set platoedcdataplane.enabled=true \
            --set platobackendapplication.enabled=true \
            --set sokratesedccontrolplane.enabled=true \
            --set sokratesedcdataplane.enabled=true \
            --set sokratesbackendapplication.enabled=true \
            --set sokrates-backend-application.persistence.enabled=true \
            --set plato-backend-application.persistence.enabled=true \
            --wait-for-jobs --timeout=120s

          # GH pipelines constrained by cpu, so give helm some time to register all resources \w k8s
          sleep 5s

          # Wait for Control-/DataPlane and backend-service to become ready
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=sokratesbackendapplication --timeout=120s || ( kubectl logs -since=0 -l app.kubernetes.io/name=sokratesbackendapplication && exit 1 )
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=platobackendapplication --timeout=120s || ( kubectl logs -l app.kubernetes.io/name=platobackendapplication && exit 1 )
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=sokratesedcdataplane --timeout=120s || ( kubectl logs -l app.kubernetes.io/name=sokratesedcdataplane && exit 1 )
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=platoedcdataplane --timeout=120s || ( kubectl logs -l app.kubernetes.io/name=platoedcdataplane && exit 1 )
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=sokratesedccontrolplane --timeout=600s || ( kubectl logs -l app.kubernetes.io/name=sokratesedccontrolplane && exit 1 )
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=platoedccontrolplane --timeout=600s || ( kubectl logs -l app.kubernetes.io/name=platoedccontrolplane && exit 1 )

      ##############################################
      ### Run Business Tests inside kind cluster ###
      ##############################################
      -
        name: Run Business Tests
        run: |-
          cat << EOF >> pod.json
          {
            "apiVersion": "v1",
            "kind": "Pod",
            "spec": {
              "containers": [
                {
                  "args": [
                    "-c",
                    "cd /product-edc && ./mvnw -s settings.xml -B -Pbusiness-tests -pl edc-tests test -Dtest=net.catenax.edc.tests.features.RunCucumberTest"
                  ],
                  "command": [
                    "/bin/sh"
                  ],
          EOF

          # Ugly hack to get env vars passed into the k8s-run - if '--overrides' defined '--env' is ignored :(
          cat << EOF >> pod.json
                  "env": [
                    {"name": "SOKRATES_DATA_MANAGEMENT_API_AUTH_KEY", "value": "${SOKRATES_DATA_MANAGEMENT_API_AUTH_KEY}"},
                    {"name": "PLATO_DATA_MANAGEMENT_API_AUTH_KEY", "value": "${PLATO_DATA_MANAGEMENT_API_AUTH_KEY}"},
                    {"name": "SOKRATES_DATA_MANAGEMENT_URL", "value": "${SOKRATES_DATA_MANAGEMENT_URL}"},
                    {"name": "SOKRATES_IDS_URL", "value": "${SOKRATES_IDS_URL}"},
                    {"name": "SOKRATES_DATA_PLANE_URL", "value": "${SOKRATES_DATA_PLANE_URL}"},
                    {"name": "PLATO_DATA_MANAGEMENT_URL", "value": "${PLATO_DATA_MANAGEMENT_URL}"},
                    {"name": "PLATO_IDS_URL", "value": "${PLATO_IDS_URL}"},
                    {"name": "PLATO_DATA_PLANE_URL", "value": "${PLATO_DATA_PLANE_URL}"}
                  ],
          EOF

          cat << EOF >> pod.json
                  "image": "openjdk:11-jdk-slim",
                  "name": "edc-tests",
                  "volumeMounts": [
                    {
                      "mountPath": "/product-edc",
                      "name": "product-edc"
                    },
                    {
                      "mountPath": "/root/.m2/repository",
                      "name": "m2-repository"
                    }
                  ]
                }
              ],
              "dnsPolicy": "ClusterFirst",
              "restartPolicy": "Never",
              "volumes": [
                {
                  "hostPath": {
                    "path": "/srv/product-edc"
                  },
                  "name": "product-edc"
                },
                {
                  "hostPath": {
                    "path": "/srv/m2-repository"
                  },
                  "name": "m2-repository"
                }
              ]
            }
          }
          EOF

          kubectl run -i --image=openjdk:11-jdk-slim --restart=Never --rm edc-tests --overrides="$(cat pod.json)"

      #################
      ### Tear Down ###
      #################
      -
        name: Destroy the kind cluster
        if: always()
        run: >-
          kind get clusters | xargs -n1 kind delete cluster --name
